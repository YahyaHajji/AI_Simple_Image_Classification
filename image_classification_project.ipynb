{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YahyaHajji/AI_Simple_Image_Classification/blob/master/image_classification_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JALJpLsQcMW-"
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# üß† DATASET CREATION WITH ADVANCED IMAGE SCRAPING\n",
        "# ====================================================\n",
        "\n",
        "!pip install duckduckgo-search pillow tensorflow scikit-learn matplotlib --quiet\n",
        "\n",
        "import os, io, requests, warnings\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from duckduckgo_search import DDGS\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"TensorFlow version:\", keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "v4lAp6ZgeTYM"
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# 1Ô∏è‚É£ Dataset structure\n",
        "# ====================================================\n",
        "\n",
        "classes = ['car', 'truck', 'frog']\n",
        "os.makedirs(\"dataset\", exist_ok=True)\n",
        "\n",
        "for cls in classes:\n",
        "    os.makedirs(f\"dataset/{cls}\", exist_ok=True)\n",
        "\n",
        "print(\"‚úì Dataset structure ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Bx1kptMJgcwt"
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# üîß Setup\n",
        "# ====================================================\n",
        "!pip install duckduckgo-search pillow requests\n",
        "\n",
        "import os, io, requests\n",
        "from PIL import Image\n",
        "from duckduckgo_search import DDGS\n",
        "\n",
        "# Define your classes\n",
        "classes = ['car', 'truck', 'frog']\n",
        "\n",
        "# Create folders for dataset\n",
        "for cls in classes:\n",
        "    os.makedirs(f'dataset/{cls}', exist_ok=True)\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# 1Ô∏è‚É£ Image fetching function\n",
        "# ====================================================\n",
        "def fetch_images(search_term, limit=60):\n",
        "    \"\"\"Fetch image URLs using DuckDuckGo safe search.\"\"\"\n",
        "    results = []\n",
        "    with DDGS() as ddgs:\n",
        "        for r in ddgs.images(\n",
        "            keywords=search_term,\n",
        "            region='wt-wt',\n",
        "            safesearch='moderate',  # ‚úÖ valid options: 'off', 'moderate', 'on'\n",
        "            max_results=limit\n",
        "        ):\n",
        "            # Extract valid image URLs\n",
        "            if \"image\" in r and r[\"image\"].startswith(\"http\"):\n",
        "                results.append(r[\"image\"])\n",
        "    return results\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# 2Ô∏è‚É£ Image validation function\n",
        "# ====================================================\n",
        "def is_image_valid(img, min_size=80):\n",
        "    \"\"\"Validate image by size, shape, and color mode.\"\"\"\n",
        "    try:\n",
        "        if img.mode != 'RGB':\n",
        "            return False\n",
        "        w, h = img.size\n",
        "        if w < min_size or h < min_size:\n",
        "            return False\n",
        "        if abs(w / h - 1) > 0.7:  # too wide/tall ratio\n",
        "            return False\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# 3Ô∏è‚É£ Download and validate images\n",
        "# ====================================================\n",
        "def download_images_advanced(classes, images_per_class=10):\n",
        "    \"\"\"Download validated images for each class until limit reached.\"\"\"\n",
        "    for cls in classes:\n",
        "        print(f\"\\nüîç Searching images for: {cls}\")\n",
        "        urls = fetch_images(cls, limit=100)\n",
        "        count = 0\n",
        "\n",
        "        for idx, url in enumerate(urls):\n",
        "            if count >= images_per_class:\n",
        "                break\n",
        "            try:\n",
        "                response = requests.get(url, timeout=8)\n",
        "                img = Image.open(io.BytesIO(response.content)).convert(\"RGB\")\n",
        "\n",
        "                if is_image_valid(img):\n",
        "                    img.save(f\"dataset/{cls}/img_{count+1}.jpg\")\n",
        "                    count += 1\n",
        "                    print(f\"‚úì Saved {cls}/img_{count}.jpg ‚úÖ\")\n",
        "                else:\n",
        "                    print(f\"‚úó Skipped (invalid image): {url}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error {idx+1}: {e}\")\n",
        "\n",
        "        print(f\"‚úÖ {count} valid images saved for '{cls}'.\")\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# 4Ô∏è‚É£ Run the scraper\n",
        "# ====================================================\n",
        "download_images_advanced(classes)\n",
        "print(\"\\nüéØ All classes now have up to 10 valid images each!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b53da123"
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Define image size and classes, ensure consistency with previous cells\n",
        "IMG_SIZE = (128, 128)\n",
        "classes = ['car', 'truck', 'frog']\n",
        "class_names = classes # Renaming for consistency with visualization cell\n",
        "class_to_idx = {cls: i for i, cls in enumerate(classes)}\n",
        "\n",
        "X = [] # To store image data\n",
        "y = [] # To store image labels\n",
        "base_dir = \"dataset\"\n",
        "\n",
        "print(\"Loading images and labels from dataset directory...\")\n",
        "for cls_name in classes:\n",
        "    class_path = os.path.join(base_dir, cls_name)\n",
        "    class_idx = class_to_idx[cls_name]\n",
        "    if os.path.exists(class_path):\n",
        "        for img_name in os.listdir(class_path):\n",
        "            img_path = os.path.join(class_path, img_name)\n",
        "            try:\n",
        "                img = Image.open(img_path).resize(IMG_SIZE).convert(\"RGB\") # Ensure RGB mode\n",
        "                img_array = np.array(img)\n",
        "                if img_array.shape == (*IMG_SIZE, 3): # Ensure correct shape for RGB images\n",
        "                    X.append(img_array)\n",
        "                    y.append(class_idx)\n",
        "                else:\n",
        "                    print(f\"Skipping {img_path}: unexpected shape {img_array.shape}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {img_path}: {e}\")\n",
        "    else:\n",
        "        print(f\"Warning: Class directory not found: {class_path}\")\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "print(f\"Loaded {len(X)} images with labels.\")\n",
        "print(f\"Shape of X: {X.shape}\")\n",
        "print(f\"Shape of y: {y.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RcjXwcs7jWxs"
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# 5Ô∏è‚É£ Split dataset into train/test folders\n",
        "# ====================================================\n",
        "\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "def split_dataset(base_dir='dataset', output_dir='data_split', train_ratio=0.8):\n",
        "    \"\"\"Split images from each class into train and test folders.\"\"\"\n",
        "    if os.path.exists(output_dir):\n",
        "        shutil.rmtree(output_dir)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for subset in ['train', 'test']:\n",
        "        for cls in os.listdir(base_dir):\n",
        "            os.makedirs(f\"{output_dir}/{subset}/{cls}\", exist_ok=True)\n",
        "\n",
        "    for cls in os.listdir(base_dir):\n",
        "        images = os.listdir(f\"{base_dir}/{cls}\")\n",
        "        random.shuffle(images)\n",
        "        split_idx = int(len(images) * train_ratio)\n",
        "        train_files = images[:split_idx]\n",
        "        test_files = images[split_idx:]\n",
        "\n",
        "        for f in train_files:\n",
        "            shutil.copy(f\"{base_dir}/{cls}/{f}\", f\"{output_dir}/train/{cls}/{f}\")\n",
        "        for f in test_files:\n",
        "            shutil.copy(f\"{base_dir}/{cls}/{f}\", f\"{output_dir}/test/{cls}/{f}\")\n",
        "\n",
        "        print(f\"‚úÖ {cls}: {len(train_files)} train | {len(test_files)} test\")\n",
        "\n",
        "    print(\"\\nüéØ Dataset successfully split into 'train' and 'test' folders!\")\n",
        "\n",
        "\n",
        "# Run the split\n",
        "split_dataset()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MskJZUkzjebA"
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# ‚úÖ Split the dataset safely + visualize samples\n",
        "# ====================================================\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"datetime.datetime.utcnow\", category=DeprecationWarning)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Check dataset balance before splitting\n",
        "class_distribution = Counter(y)\n",
        "print(\"üìä Class distribution before split:\", class_distribution)\n",
        "\n",
        "# Ensure that each class has at least 2 samples\n",
        "if min(class_distribution.values()) < 2:\n",
        "    print(\"‚ö†Ô∏è Not enough samples per class ‚Äî disabling stratify for safety.\")\n",
        "    stratify_param = None\n",
        "else:\n",
        "    stratify_param = y\n",
        "\n",
        "# Split dataset into training (80%) and validation (20%)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=stratify_param\n",
        ")\n",
        "\n",
        "# Convert labels to categorical (one-hot encoding)\n",
        "num_classes = len(class_names)\n",
        "y_train_cat = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_val_cat = keras.utils.to_categorical(y_val, num_classes)\n",
        "\n",
        "print(f\"‚úÖ Train set: {X_train.shape[0]} images\")\n",
        "print(f\"‚úÖ Validation set: {X_val.shape[0]} images\")\n",
        "print(f\"‚úÖ Classes: {class_names}\")\n",
        "print(f\"‚úÖ Image shape: {X_train.shape[1:]}\")\n",
        "\n",
        "# ====================================================\n",
        "# üñºÔ∏è Visualize some random samples from the training set\n",
        "# ====================================================\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "indices = np.random.choice(len(X_train), 6, replace=False)\n",
        "\n",
        "for i, idx in enumerate(indices):\n",
        "    plt.subplot(2, 3, i+1)\n",
        "    plt.imshow(X_train[idx])\n",
        "    plt.title(class_names[y_train[idx]])\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "782Q7a3Kjib9"
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# 7Ô∏è‚É£ Load dataset and prepare data generators\n",
        "# ====================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "IMG_SIZE = (128, 128)\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_dir = \"data_split/train\"\n",
        "test_dir = \"data_split/test\"\n",
        "\n",
        "# Data augmentation for better generalization\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_gen = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "num_classes = len(train_gen.class_indices)\n",
        "print(f\"\\nüìö Detected {num_classes} classes: {list(train_gen.class_indices.keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7dkZob6ejlSM"
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# 8Ô∏è‚É£ Define a simple CNN model\n",
        "# ====================================================\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(*IMG_SIZE, 3)),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dropout(0.3),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 9Ô∏è‚É£ Train the CNN model\n",
        "# ====================================================\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=test_gen,\n",
        "    epochs=EPOCHS\n",
        ")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tNk9xUgXyUSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# üîü Visualize accuracy and loss\n",
        "# ====================================================\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
        "plt.title('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zj6SxOYUyWaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 11Ô∏è‚É£ Test predictions on random test images\n",
        "# ====================================================\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class_names = list(train_gen.class_indices.keys())\n",
        "\n",
        "def show_random_predictions(n=5):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    for i in range(n):\n",
        "        cls = random.choice(class_names)\n",
        "        img_name = random.choice(os.listdir(f\"{test_dir}/{cls}\"))\n",
        "        img_path = f\"{test_dir}/{cls}/{img_name}\"\n",
        "\n",
        "        img = Image.open(img_path).resize(IMG_SIZE)\n",
        "        img_array = np.expand_dims(np.array(img) / 255.0, axis=0)\n",
        "        pred = model.predict(img_array, verbose=0)\n",
        "        pred_cls = class_names[np.argmax(pred)]\n",
        "\n",
        "        plt.subplot(1, n, i+1)\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"True: {cls}\\nPred: {pred_cls}\")\n",
        "    plt.show()\n",
        "\n",
        "show_random_predictions()\n"
      ],
      "metadata": {
        "id": "TFbk0lfzyYT8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOh8qqL1l9NnToqXVAEjILY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}